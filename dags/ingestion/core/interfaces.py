from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Dict, Iterator, List, Optional
import pandas as pd

@dataclass
class IngestionJob:
    """
    Represents a single unit of work generated by the Partitioner.
    
    Attributes:
        params: The dictionary of arguments passed to the Extractor (e.g., {'symbol': 'AAPL', 'start_date': ...})
        meta: Optional metadata used for tracking or compaction (e.g., {'shard_id': '2023-01'})
    """
    params: Dict[str, Any]
    meta: Dict[str, Any] = field(default_factory=dict)

class BasePartitioner(ABC):
    """
    Responsible for splitting a large scope into smaller execution jobs.
    """
    
    @abstractmethod
    def generate_jobs(
        self, 
        start_date: Optional[str] = None, 
        end_date: Optional[str] = None, 
        **kwargs
    ) -> Iterator[IngestionJob]:
        """
        Generates a stream of jobs based on the time range and configuration.
        """
        pass

class BaseExtractor(ABC):
    """
    Responsible for executing a single job and returning raw data.
    """
    
    @abstractmethod
    def extract(self, job: IngestionJob) -> Optional[pd.DataFrame]:
        """
        Executes the job. Returns a DataFrame or None if no data found.
        
        Args:
            job: The job context containing params needed for extraction.
        """
        pass

class BaseCompactor(ABC):
    """
    Responsible for persisting the extracted data into the final storage.
    Handles deduplication, schema validation, and atomic writes.
    """
    
    @abstractmethod
    def compact(
        self, 
        results: List[Any], 
        target: str, 
        partition_date: str,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Compacts the results.
        
        Args:
            results: A list of outputs from the Extractor (e.g., list of temporary file paths or DataFrames).
                     Note: In a distributed Airflow setup, passing DataFrames directly in XCom is bad practice.
                     Ideally, Extractor returns a path to a staging file, and Compactor reads it.
                     For simplicity in single-machine/local setups, DataFrames are accepted but discouraged for large data.
            target: The name of the dataset (e.g., 'fund_etf_history').
            partition_date: The logical date of this run.
        
        Returns:
            Metrics about the compaction (e.g., {'row_count': 100, 'file_size': 1024}).
        """
        pass
