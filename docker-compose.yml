services:
  # postgres:
  #     image: postgres:13
  #     container_name: airflow-postgres
  #     environment:
  #       - POSTGRES_USER=airflow
  #       - POSTGRES_PASSWORD=airflow
  #       - POSTGRES_DB=airflow
  #     volumes:
  #       - postgres-db-volume:/var/lib/postgresql/data
  #     ports:
  #       - "5432:5432" # 暴露端口，方便你用 Navicat/DBeaver 连接调试
  #     healthcheck:
  #       test: ["CMD", "pg_isready", "-U", "airflow"]
  #       interval: 10s
  #       retries: 5
  #     restart: always
  # airflow-webserver:
  #   build:
  #     context: .
  #     dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
  #   container_name: airflow-webserver
  #   restart: always
  #   env_file:
  #     - .env
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=False
  #     - AIRFLOW__CORE__PARALLELISM=6
  #     - AIRFLOW__CORE__DAG_CONCURRENCY=1
  #     - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=4
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
  #     - AIRFLOW_CONN_MINIO_S3=${AIRFLOW_CONN_MINIO_S3}
  #     - AIRFLOW__WEBSERVER__WORKERS=1
  #     - PYTHONPATH=/opt/airflow
  #     # - AIRFLOW__WEBSERVER__UPDATE_FAB_PERMS=False
  #     # - AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=300
  #     # - AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE=0
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./lakehouse_core:/opt/airflow/lakehouse_core
  #     - ./catalog:/opt/airflow/catalog
  #     - ./.duckdb:/opt/airflow/.duckdb
  #     - ./logs:/opt/airflow/logs
  #   ports:
  #     - "8080:8080"
  #   depends_on:
  #     airflow-init:
  #       condition: service_completed_successfully
  #     postgres:
  #       condition: service_healthy
  #   command: webserver
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  # airflow-scheduler:
  #   build:
  #     context: .
  #     dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
  #   container_name: airflow-scheduler
  #   restart: always
  #   env_file:
  #     - .env
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  #     - AIRFLOW__CORE__PARALLELISM=6
  #     - AIRFLOW__CORE__DAG_CONCURRENCY=1
  #     - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=4
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
  #     - AIRFLOW_CONN_MINIO_S3=${AIRFLOW_CONN_MINIO_S3}
  #     - PYTHONPATH=/opt/airflow
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./lakehouse_core:/opt/airflow/lakehouse_core
  #     - ./catalog:/opt/airflow/catalog
  #     - ./.duckdb:/opt/airflow/.duckdb
  #     - ./logs:/opt/airflow/logs
  #   depends_on:
  #     airflow-init:
  #       condition: service_completed_successfully
  #     postgres:
  #       condition: service_healthy
  #   command: scheduler
  #   healthcheck:  
  #     test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  # airflow-triggerer:
  #   build:
  #     context: .
  #     dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
  #   container_name: airflow-triggerer
  #   restart: always
  #   env_file:
  #     - .env
  #   environment:
  #     - AIRFLOW__CORE__PARALLELISM=6
  #     - AIRFLOW__CORE__DAG_CONCURRENCY=1
  #     - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=4
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
  #     - AIRFLOW_CONN_MINIO_S3=${AIRFLOW_CONN_MINIO_S3}
  #     - PYTHONPATH=/opt/airflow
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./lakehouse_core:/opt/airflow/lakehouse_core
  #     - ./catalog:/opt/airflow/catalog
  #     - ./.duckdb:/opt/airflow/.duckdb
  #     - ./logs:/opt/airflow/logs
  #   depends_on:
  #     airflow-init:
  #       condition: service_completed_successfully
  #     postgres:
  #       condition: service_healthy
  #   command: triggerer
  #   healthcheck:
  #     test: ["CMD", "airflow", "jobs", "check", "--job-type", "TriggererJob"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  # airflow-init:
  #   build:
  #     context: .
  #     dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
  #   container_name: airflow-init
  #   env_file:
  #     - .env
  #   environment:
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
  #     - PYTHONPATH=/opt/airflow
  #   command: >
  #     bash -c "
  #     airflow db migrate &&
  #     airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password ${AIRFLOW_ADMIN_PASSWORD:-admin}
  #     "
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./lakehouse_core:/opt/airflow/lakehouse_core
  #     - ./catalog:/opt/airflow/catalog
  #     - ./.duckdb:/opt/airflow/.duckdb
  #     - ./logs:/opt/airflow/logs

  minio:
    image: quay.io/minio/minio
    container_name: minio-stock
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
    # depends_on:
    #   postgres:
    #     condition: service_healthy

  prefect-server:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.prefect
    container_name: prefect-server
    restart: always
    environment:
      - PREFECT_SERVER_API_HOST=0.0.0.0
      - PREFECT_UI_API_URL=http://localhost:4200/api
      - PREFECT_API_URL=http://localhost:4200/api
      - PREFECT_API_DATABASE_CONNECTION_URL=sqlite+aiosqlite:////root/.prefect/prefect.db
    volumes:
      - prefect-data:/root/.prefect
    ports:
      - "4200:4200"
    command: prefect server start
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4200/api/health')"]
      interval: 10s
      timeout: 5s
      retries: 12

  prefect-worker:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.prefect
    container_name: prefect-worker
    restart: always
    env_file:
      - .env
    environment:
      - PREFECT_API_URL=http://prefect-server:4200/api
      - PYTHONPATH=/opt/prefect:/opt/prefect/prefect
    volumes:
      - ./prefect:/opt/prefect/prefect
      - ./lakehouse_core:/opt/prefect/lakehouse_core
      - ./dags:/opt/prefect/dags
      - ./catalog:/opt/prefect/catalog
      - ./.duckdb:/opt/prefect/.duckdb
      - ./logs:/opt/prefect/logs
    depends_on:
      prefect-server:
        condition: service_healthy
      minio:
        condition: service_healthy
    command: prefect worker start --pool default

  prefect-deploy:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.prefect
    container_name: prefect-deploy
    env_file:
      - .env
    environment:
      - PREFECT_API_URL=http://prefect-server:4200/api
      - PYTHONPATH=/opt/prefect:/opt/prefect/prefect
    volumes:
      - ./prefect:/opt/prefect/prefect
      - ./lakehouse_core:/opt/prefect/lakehouse_core
      - ./dags:/opt/prefect/dags
      - ./catalog:/opt/prefect/catalog
      - ./.duckdb:/opt/prefect/.duckdb
      - ./logs:/opt/prefect/logs
    depends_on:
      prefect-server:
        condition: service_healthy
    command: >
      bash -c "cd /opt/prefect/prefect && prefect deploy --all && python /opt/prefect/prefect/flows/register_dw_layer_deployments.py"

  # jupyter:
  #   build:
  #     context: .
  #     dockerfile: ./infra_build/dockerfiles/Dockerfile.jupyter
  #   container_name: jupyter-stock
  #   ports:
  #     - "8888:8888"
  #   environment:
  #     - S3_ENDPOINT_URL=${S3_ENDPOINT_URL}
  #     - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
  #     - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
  #     - S3_BUCKET_NAME=${S3_BUCKET_NAME}
  #     - JUPYTER_TOKEN=admin
  #   volumes:
  #     - ./notebooks:/home/jovyan/work

volumes:
  postgres-db-volume:
  prefect-data:
