services:
  postgres:
      image: postgres:13
      container_name: airflow-postgres
      environment:
        - POSTGRES_USER=airflow
        - POSTGRES_PASSWORD=airflow
        - POSTGRES_DB=airflow
      volumes:
        - postgres-db-volume:/var/lib/postgresql/data
      ports:
        - "5432:5432" # 暴露端口，方便你用 Navicat/DBeaver 连接调试
      healthcheck:
        test: ["CMD", "pg_isready", "-U", "airflow"]
        interval: 10s
        retries: 5
      restart: always
  airflow-webserver:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
    container_name: airflow-webserver
    restart: always
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__PARALLELISM=6
      - AIRFLOW__CORE__DAG_CONCURRENCY=1
      - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=4
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
      - AIRFLOW_CONN_MINIO_S3=${AIRFLOW_CONN_MINIO_S3}
      - AIRFLOW__WEBSERVER__WORKERS=1
      - PYTHONPATH=/opt/airflow
      # - AIRFLOW__WEBSERVER__UPDATE_FAB_PERMS=False
      # - AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=300
      # - AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE=0
    volumes:
      - ./dags:/opt/airflow/dags
      - ./lakehouse_core:/opt/airflow/lakehouse_core
      - ./catalog:/opt/airflow/catalog
      - ./.duckdb:/opt/airflow/.duckdb
      - ./logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
    container_name: airflow-scheduler
    restart: always
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__PARALLELISM=6
      - AIRFLOW__CORE__DAG_CONCURRENCY=1
      - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=4
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
      - AIRFLOW_CONN_MINIO_S3=${AIRFLOW_CONN_MINIO_S3}
      - PYTHONPATH=/opt/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./lakehouse_core:/opt/airflow/lakehouse_core
      - ./catalog:/opt/airflow/catalog
      - ./.duckdb:/opt/airflow/.duckdb
      - ./logs:/opt/airflow/logs
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    command: scheduler
    healthcheck:  
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-triggerer:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
    container_name: airflow-triggerer
    restart: always
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__PARALLELISM=6
      - AIRFLOW__CORE__DAG_CONCURRENCY=1
      - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=4
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
      - AIRFLOW_CONN_MINIO_S3=${AIRFLOW_CONN_MINIO_S3}
      - PYTHONPATH=/opt/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./lakehouse_core:/opt/airflow/lakehouse_core
      - ./catalog:/opt/airflow/catalog
      - ./.duckdb:/opt/airflow/.duckdb
      - ./logs:/opt/airflow/logs
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    command: triggerer
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "TriggererJob"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-init:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
    container_name: airflow-init
    env_file:
      - .env
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
      - PYTHONPATH=/opt/airflow
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password ${AIRFLOW_ADMIN_PASSWORD:-admin}
      "
    volumes:
      - ./dags:/opt/airflow/dags
      - ./lakehouse_core:/opt/airflow/lakehouse_core
      - ./catalog:/opt/airflow/catalog
      - ./.duckdb:/opt/airflow/.duckdb
      - ./logs:/opt/airflow/logs

  minio:
    image: quay.io/minio/minio
    container_name: minio-stock
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      postgres:
        condition: service_healthy

  jupyter:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.jupyter
    container_name: jupyter-stock
    restart: always
    env_file:
      - .env
    environment:
      - S3_ENDPOINT_URL=${S3_ENDPOINT_URL:-http://minio:9000}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID:-minioadmin}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY:-minioadmin}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME:-stock-data}
      - S3_REGION=${S3_REGION:-us-east-1}
      - DUCKDB_CATALOG_PATH=/home/jovyan/.duckdb/catalog.duckdb
      - JUPYTER_TOKEN=${JUPYTER_TOKEN:-admin}
      - PYTHONPATH=/home/jovyan:/home/jovyan/lakehouse_core
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./lakehouse_core:/home/jovyan/lakehouse_core
      - ./.duckdb:/home/jovyan/.duckdb
    ports:
      - "8888:8888"
    depends_on:
      minio:
        condition: service_healthy
      postgres:
        condition: service_healthy
    command: >
      bash -c "
      mkdir -p /home/jovyan/.duckdb &&
      start-notebook.sh --NotebookApp.token=${JUPYTER_TOKEN:-admin} --NotebookApp.password=''
      "
volumes:
  postgres-db-volume:
