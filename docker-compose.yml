services:
  postgres:
      image: postgres:13
      container_name: airflow-postgres
      environment:
        - POSTGRES_USER=airflow
        - POSTGRES_PASSWORD=airflow
        - POSTGRES_DB=airflow
      volumes:
        - postgres-db-volume:/var/lib/postgresql/data
      ports:
        - "5432:5432" # 暴露端口，方便你用 Navicat/DBeaver 连接调试
      healthcheck:
        test: ["CMD", "pg_isready", "-U", "airflow"]
        interval: 10s
        retries: 5
      restart: always
  airflow-webserver:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
    container_name: airflow-webserver
    restart: always
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__PARALLELISM=6
      - AIRFLOW__CORE__DAG_CONCURRENCY=1
      - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=4
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
      - AIRFLOW_CONN_MINIO_S3=${AIRFLOW_CONN_MINIO_S3}
      - AIRFLOW__WEBSERVER__WORKERS=1
      - PYTHONPATH=/opt/airflow
      # - AIRFLOW__WEBSERVER__UPDATE_FAB_PERMS=False
      # - AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=300
      # - AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE=0
    volumes:
      - ./dags:/opt/airflow/dags
      - ./catalog:/opt/airflow/catalog
      - ./.duckdb:/opt/airflow/.duckdb
      - ./logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
    container_name: airflow-scheduler
    restart: always
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__PARALLELISM=6
      - AIRFLOW__CORE__DAG_CONCURRENCY=1
      - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=4
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
      - AIRFLOW_CONN_MINIO_S3=${AIRFLOW_CONN_MINIO_S3}
      - PYTHONPATH=/opt/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./catalog:/opt/airflow/catalog
      - ./.duckdb:/opt/airflow/.duckdb
      - ./logs:/opt/airflow/logs
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    command: scheduler
    healthcheck:  
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-triggerer:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
    container_name: airflow-triggerer
    restart: always
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__PARALLELISM=6
      - AIRFLOW__CORE__DAG_CONCURRENCY=1
      - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=4
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
      - AIRFLOW_CONN_MINIO_S3=${AIRFLOW_CONN_MINIO_S3}
      - PYTHONPATH=/opt/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./catalog:/opt/airflow/catalog
      - ./.duckdb:/opt/airflow/.duckdb
      - ./logs:/opt/airflow/logs
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    command: triggerer
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "TriggererJob"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-init:
    build:
      context: .
      dockerfile: ./infra_build/dockerfiles/Dockerfile.airflow
    container_name: airflow-init
    env_file:
      - .env
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_METADATA_DB_URL}
      - PYTHONPATH=/opt/airflow
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password ${AIRFLOW_ADMIN_PASSWORD:-admin}
      "
    volumes:
      - ./dags:/opt/airflow/dags
      - ./catalog:/opt/airflow/catalog
      - ./.duckdb:/opt/airflow/.duckdb
      - ./logs:/opt/airflow/logs

  minio:
    image: quay.io/minio/minio
    container_name: minio-stock
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      postgres:
        condition: service_healthy

  # jupyter:
  #   build:
  #     context: .
  #     dockerfile: ./infra_build/dockerfiles/Dockerfile.jupyter
  #   container_name: jupyter-stock
  #   ports:
  #     - "8888:8888"
  #   environment:
  #     - S3_ENDPOINT_URL=${S3_ENDPOINT_URL}
  #     - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
  #     - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
  #     - S3_BUCKET_NAME=${S3_BUCKET_NAME}
  #     - JUPYTER_TOKEN=admin
  #   volumes:
  #     - ./notebooks:/home/jovyan/work

volumes:
  postgres-db-volume:
